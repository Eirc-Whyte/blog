---
title: 建模-主成分分析
date: 2020-08-20 23:29:48
tags:
- 综合评价方法
- 主成分分析
categories:
- 数学建模
---

### 主成分分析（PCA）

​	主成分分析（Principal components analysis，以下简称PCA）是最重要的降维方法之一。在数据压缩消除冗余和数据噪音消除等领域都有广泛的应用。一般我们提到降维最容易想到的算法就是PCA。

<!--more-->

​	顾名思义，PCA就是找出数据里最主要的方面，用数据里最主要的方面来代替原始数据。具体的，假如我们的数据集是n维的，共有m个数据$(x^{(1)},x^{(2)},...,x^{(m)})$。我们希望将这$m$个数据的维度从$n$维降到$n'$维，希望这$m$个$n'$维的数据集尽可能的代表原始数据集。我们知道数据从n维降到n'维肯定会有损失，但是我们希望损失尽可能的小。那么如何让这$n'$维的数据尽可能表示原来的数据呢？

​	我们先看看最简单的情况，也就是$n=2，n'=1$,也就是将数据从二维降维到一维。数据如下图。我们希望找到某一个维度方向，它可以代表这两个维度的数据。图中列了两个向量方向，$u1$和$u2$，那么哪个向量可以更好的代表原始数据集呢？从直观上也可以看出，$u1$比$u2$好。

![img](https://ericblog.oss-cn-beijing.aliyuncs.com/img/1042406-20161231162149992-1521335659.png)

　　　　为什么$u1$比$u2$好呢？可以有两种解释，第一种解释是样本点到这个直线的**距离足够近**，第二种解释是样本点在这个直线上的**投影能尽可能的分开**。

　　　　假如我们把$n'$从1维推广到任意维，则我们的希望降维的标准为：**样本点到这个超平面的距离足够近,或者说样本点在这个超平面上的投影能尽可能的分开。**

### PCA推导

- 基于最小投影距离
- 基于最大投影方差

- 基于奇异值分解（SVD）

### PCA流程

输入：n维样本集$D=(x^{(1)}, x^{(2)},...,x^{(m)})$，要降维到的维数$n'$.

输出：降维后的样本集$D′$

1) 对所有的样本进行中心化： $x^{(i)} = x^{(i)} - \frac{1}{m}\sum\limits_{j=1}^{m} x^{(j)}$

2) 计算样本的协方差矩阵$XX^T$

3) 对矩阵$XX^T$进行特征值分解

4）取出最大的$n'$个特征值对应的特征向量$(w_1,w_2,...,w_{n'})$ 将所有的特征向量标准化后，组成特征向量矩阵$W$。

5）对样本集中的每一个样本$x^{(i)}$,转化为新的样本$z^{(i)}=W^Tx^{(i)}$

6) 得到输出样本集$D' =(z^{(1)}, z^{(2)},...,z^{(m)})$

​	有时候，我们**不指定**降维后的$n'$的值，而是换种方式，指定一个降维到的**主成分比重阈值$t$**。这个阈值$t$在$(0,1]$之间.假如我们的$n$个特征值为$\lambda_1 \geq \lambda_2 \geq ... \geq \lambda_n$,则$n'$可以通过下式得到:
$$
\frac{\sum\limits_{i=1}^{n'}\lambda_i}{\sum\limits_{i=1}^{n}\lambda_i} \geq t
$$

### PCA优缺点

PCA算法的**主要优点**有：

1）仅仅需要以方差衡量信息量，不受数据集以外的因素影响。　

2）各主成分之间正交，可消除原始数据成分间的相互影响的因素。

3）计算方法简单，主要运算是特征值分解，易于实现。

PCA算法的**主要缺点**有：

1）主成分各个特征维度的含义具有一定的**模糊性**，不如原始样本特征的解释性强。

2）方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。

> 参考博客：
>
> [主成分分析（PCA）原理总结](https://www.cnblogs.com/pinard/p/6239403.html)
>
> [从特征分解，奇异值分解到主成分分析](https://www.cnblogs.com/duye/p/10731816.html)