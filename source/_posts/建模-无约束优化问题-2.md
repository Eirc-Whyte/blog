---
title: 建模-无约束优化问题-2
date: 2020-08-22 22:42:02
tags:
- 优化问题
- 多维搜索方法
- 梯度下降法
- 牛顿法
categories:
- 数学建模
---

### 二维搜索方法

本文描述了求取实值函数极小点的方法。主要包括最速下降法、牛顿法、拟牛顿法。

<!--more-->

#### 梯度下降法

又叫最速下降法，是梯度方法的一种具体实现。在目标函数一阶可导时，应利用导数（梯度）的信息， 向负梯度方向搜索前进， 使得每一步的目标函数值都减小。

具体方法是：每次从初始点出发，沿着**梯度负方向$-\nabla f(x^{(k)})$开展一维搜索**，找到该方向上的**极小值点**$x^{(k+1)}$，再从此点继续迭代搜索。

![image-20200822224937855](https://ericblog.oss-cn-beijing.aliyuncs.com/img/image-20200822224937855.png)

算法优化：

1. **算法的步长选择**。步长取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。也可以选择相对值作为停止规则，以**避免数据尺度不同**造成的影响。例如使用：
   $$
   \frac{|f(x^{(k+1)})-f(x^{(k)})|}{|f(x^{(k)})|}<\epsilon
   $$
   或者
   $$
   \frac{|x^{(k+1)}-x^{(k)}|}{|x^{(k)}|}<\epsilon
   $$
   

2. **算法参数的初始值选择**。初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要**多次用不同初始值**运行算法，关键损失函数的最小值，选择损失函数最小化的初值。

3. **归一化**。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了**减少特征取值**的影响，可以对特征数据归一化。

#### 牛顿法

在确定搜索方向时，梯度下降法只是用了一阶导数（梯度），如果函数存在二阶以上导数时，引入高阶导数，则效率可能提高。

原理是利用泰勒公式，在$x_0$处进行二阶泰勒展开。

![image-20200822232309382](https://ericblog.oss-cn-beijing.aliyuncs.com/img/image-20200822232309382.png)

#### 拟牛顿法

> ref：
>
> [无约束优化方法](https://www.math.pku.edu.cn/teachers/lidf/docs/statcomp/html/_statcompbook/opt-uncons.html#opt-uncons-quaisnewton)
>
> [梯度下降法、牛顿法和拟牛顿法](https://zhuanlan.zhihu.com/p/37524275)

